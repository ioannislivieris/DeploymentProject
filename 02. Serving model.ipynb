{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate server\n",
    "#\n",
    "# $ docker-compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Basic libraries\n",
    "#\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Scipy library\n",
    "#\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Triton client libraries\n",
    "#\n",
    "import tritonclient\n",
    "import tritonclient.grpc                  as triton_grpc  \n",
    "import tritonclient.grpc.model_config_pb2 as mc\n",
    "from   tritonclient.utils                 import triton_to_np_dtype\n",
    "from   tritonclient.utils                 import InferenceServerException\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# User libraries\n",
    "#\n",
    "from utils.Logger        import *\n",
    "from utils.InfuxDBClient import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST    = 'localhost'\n",
    "PORT    = 6001\n",
    "TIMEOUT = 60\n",
    "\n",
    "\n",
    "PATH     = os.path.abspath('model_repository')\n",
    "\n",
    "\n",
    "# Model names\n",
    "#\n",
    "model_name  = 'AnomalyDetectionModel'\n",
    "\n",
    "\n",
    "# Input & output names as well as models' version is the same\n",
    "#\n",
    "input_name    = 'InputLayer'\n",
    "output_name   = 'OutputLayer'\n",
    "model_version = '1'\n",
    "\n",
    "\n",
    "# Selected sensor\n",
    "#\n",
    "Sensor = '01cc0529-79bb-48cc-a052-4fde9372abeb'\n",
    "\n",
    "# Set look-back (Lag)\n",
    "#\n",
    "Lag = 32\n",
    "\n",
    "# Set 'verbose'\n",
    "#\n",
    "VERBOSE = True\n",
    "\n",
    "\n",
    "url = '{}:{}'.format(HOST, PORT )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate logger\n",
    "#\n",
    "if VERBOSE:\n",
    "    logger = init_logger( log_file = 'logs.log' )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] TRITON server is ready\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_server_ready, metadata ()\n",
      "\n",
      "ready: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ------------------------------------------------------------------------- ### \n",
    "## setup TRT\n",
    "### ------------------------------------------------------------------------- ### \n",
    "#\n",
    "triton_client  = triton_grpc.InferenceServerClient(url     = url, \n",
    "                                                   verbose = VERBOSE)\n",
    "\n",
    "\n",
    "# Wait for server to come online\n",
    "server_start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        if triton_client.is_server_ready() or time.time() - server_start > TIMEOUT:\n",
    "            if VERBOSE: \n",
    "                logger.info('[INFO] TRITON server is ready')\n",
    "            break\n",
    "        \n",
    "    except InferenceServerException:\n",
    "        if VERBOSE: \n",
    "            logger.error('[ERROR] TRITON server is not ready')\n",
    "        pass\n",
    "   \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from InfuxDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup InfuxDB Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time limit\n",
    "#\n",
    "LIMIT = 10 * 32\n",
    "\n",
    "\n",
    "influx_measurement = \"beat_box_rms\"\n",
    "\n",
    "\n",
    "# Change to influx live data \n",
    "#\n",
    "influx = InfluxDataframeDBClient(host     = '192.168.2.198', \n",
    "                                 username = 'admin',\n",
    "                                 pwd      = 'CoreInn02019', \n",
    "                                 port     =  8086, \n",
    "                                 db       = 'corebeat_rms')\n",
    "influx.initClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Download data from CoreBEAT database\n",
      "[INFO] Time: 0.04s\n",
      "[INFO] Data cleaning - Data processing\n",
      "[INFO] Number of features: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-07-07 10:50:01+00:00</th>\n",
       "      <td>0.574637</td>\n",
       "      <td>0.701521</td>\n",
       "      <td>0.957487</td>\n",
       "      <td>37.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-07 10:51:00+00:00</th>\n",
       "      <td>0.194573</td>\n",
       "      <td>0.171073</td>\n",
       "      <td>0.206829</td>\n",
       "      <td>37.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-07 10:52:00+00:00</th>\n",
       "      <td>0.228922</td>\n",
       "      <td>0.227197</td>\n",
       "      <td>0.343287</td>\n",
       "      <td>37.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              acc_x     acc_y     acc_z  temperature\n",
       "2022-07-07 10:50:01+00:00  0.574637  0.701521  0.957487         37.3\n",
       "2022-07-07 10:51:00+00:00  0.194573  0.171073  0.206829         37.3\n",
       "2022-07-07 10:52:00+00:00  0.228922  0.227197  0.343287         37.4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start timer\n",
    "#\n",
    "start_timer = time.time()\n",
    "\n",
    "# TABLE: corebeat_rms\n",
    "#\n",
    "df = influx.client.query(f'SELECT * from {influx_measurement} order by time DESC limit {LIMIT}') \n",
    "df = df[ influx_measurement ]\n",
    "#\n",
    "if VERBOSE: \n",
    "    logger.info('[INFO] Download data from CoreBEAT database')\n",
    "    logger.info('[INFO] Time: {:.2f}s'.format(time.time() - start_timer))\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "#\n",
    "df = df[ df['beat_box_id'] == Sensor]\n",
    "\n",
    "# \n",
    "del df['beat_box_id']\n",
    "\n",
    "\n",
    "# Get only the last -Lag- required measurements\n",
    "#\n",
    "df = df[-Lag:]\n",
    "\n",
    "\n",
    "if VERBOSE: \n",
    "    logger.info('[INFO] Data cleaning - Data processing')\n",
    "    logger.info('[INFO] Number of features: {}'.format(df.shape[1]))\n",
    "    \n",
    "df.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create instance for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Instance created\n",
      "[INFO] Number of features: 4\n"
     ]
    }
   ],
   "source": [
    "Instance = df.to_numpy()\n",
    "\n",
    "if VERBOSE: \n",
    "    logger.info('[INFO] Instance created')\n",
    "    logger.info('[INFO] Number of features: {}'.format(Instance.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess( Instance, model_name ):\n",
    "    \n",
    "    # Load Scaler\n",
    "    #\n",
    "    scaler = pickle.load( open( '{}/{}/Scaler.pkl'.format(PATH, model_name), 'rb'))    \n",
    "    \n",
    "    # Apply scaler\n",
    "    #\n",
    "    Instance = scaler.transform( Instance )\n",
    "    \n",
    "    # Add dimension\n",
    "    #\n",
    "    Instance = np.expand_dims(Instance, axis=0)\n",
    "    \n",
    "    return Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/livieris/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "[INFO] Instance was preprocessed\n"
     ]
    }
   ],
   "source": [
    "Instance = Preprocess( Instance, model_name )\n",
    "\n",
    "\n",
    "if VERBOSE: \n",
    "    logger.info('[INFO] Instance was preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(data, model_name, input_name, output_name, verbose = False):\n",
    "\n",
    "    # Setup inputs for inference\n",
    "    #\n",
    "    modelInputs = triton_grpc.InferInput(input_name, shape=[1, 32, 4], datatype='FP32');\n",
    "    modelInputs.set_data_from_numpy( data.astype('float32') );\n",
    "    \n",
    "    \n",
    "    # Setup output for inference\n",
    "    #\n",
    "    modelOutputs = triton_grpc.InferRequestedOutput( output_name );\n",
    "\n",
    "    \n",
    "    # Get response\n",
    "    #\n",
    "    response = triton_client.infer(model_name, \n",
    "                                   model_version = model_version, \n",
    "                                   inputs        = [ modelInputs  ], \n",
    "                                   outputs       = [ modelOutputs ]);\n",
    "    \n",
    "\n",
    "    # Process response\n",
    "    #\n",
    "    logits = response.as_numpy( output_name );\n",
    "    logits = np.asarray(logits, dtype=np.float32);\n",
    "\n",
    "    if (verbose):\n",
    "        logger.info(f\"[INFO] Infer with TRITON server: {model_name}\");\n",
    "        logger.info(f\"[INFO] Prediction shape: {logits.shape} \");\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Get predictions\n",
    "    #\n",
    "    pred    = np.array(logits)\n",
    "\n",
    "    \n",
    "    # Calculate losses\n",
    "    #\n",
    "    mae_loss = np.mean( np.mean(np.abs(pred - data), axis = 2), axis = 1)\n",
    "    mse_loss = np.mean(np.mean((pred - data)**2, axis=-1), axis = 1)\n",
    "    if (verbose):\n",
    "        logger.info('[INFO] MAE & MSE loss calculated')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    # Clean garbage\n",
    "    #\n",
    "    del modelInputs, response\n",
    "    del logits\n",
    "    gc.collect();\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Create DataFrame with output/response\n",
    "    #\n",
    "    df_response = pd.DataFrame()\n",
    "    \n",
    "    # Store losses\n",
    "    #\n",
    "    df_response['MSE loss'] = mse_loss\n",
    "    df_response['MAE loss'] = mae_loss\n",
    "\n",
    "    if (verbose): \n",
    "        logger.info(f\"Output df: {df_response.head(3)} \")\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer, metadata ()\n",
      "model_name: \"AnomalyDetectionModel\"\n",
      "model_version: \"1\"\n",
      "inputs {\n",
      "  name: \"InputLayer\"\n",
      "  datatype: \"FP32\"\n",
      "  shape: 1\n",
      "  shape: 32\n",
      "  shape: 4\n",
      "}\n",
      "outputs {\n",
      "  name: \"OutputLayer\"\n",
      "}\n",
      "raw_input_contents: \",\\240\\306@\\311\\252\\013A\\224\\350\\342@\\2401\\033@2B2=W\\204\\224\\272 \\317\\256=\\2401\\033@\\252\\276\\031?=.l?\\326\\356\\255?\\334i @r\\266$\\277M@\\374\\276\\221X\\222\\275\\2401\\033@\\326 (\\277\\372\\341\\317\\276c\\177\\362\\275\\334i @\\376\\207\\303@{{\\016A9.\\005A\\2401\\033@g\\224\\014\\277|,;\\275\\212{\\321\\275\\2401\\033@\\205H\\204?\\270\\037\\370?(e\\355?\\2401\\033@\\340\\373&\\277\\235\\237\\252\\276&~ >\\2401\\033@\\377\\362\\217\\277\\0312x\\277\\354+\\024\\277\\334i @\\330\\374\\264@\\026A\\322@\\363O\\337@\\2401\\033@:sc\\276X\\r\\313=1(\\215>\\2401\\033@\\265\\234<\\276m\\007?\\276\\365\\345\\331=(\\301\\020@8>\\276?\\212\\263\\371?k\\310\\375?(\\301\\020@\\251\\370_:\\\\o\\231<\\210\\263H\\273d\\371\\025@\\255\\020X?@b\\306?\\375\\212\\230?d\\371\\025@\\351EL\\275\\312\\032\\216\\275\\'\\352{\\276d\\371\\025@\\351\\300A@\\036\\232[@.\\362U@d\\371\\025@\\313%5@\\024AL@\\275\\351l@\\2401\\033@.\\337\\253@\\2272\\313@<\\014\\306@\\030\\242%@\\0023\\217?d\\360\\204?\\342\\360\\235?\\334i @c\\326\\227>\\217\\203\\204>\\316\\362\\303\\276\\030\\242%@+w3?\\212\\204\\233>l\\321B=\\030\\242%@\\007\\243S?\\2614/?\\335\\275\\211>\\030\\242%@\\233[\\325?\\335\\361\\346?R\\255\\301?\\217\\0220@q8K@\\265\\215t@\\227\\223\\202@\\030\\242%@\\3510\\202?U\\035\\037?\\264\\230\\004?\\334i @|\\206\\356@\\200q\\005A0/\\nA\\030\\242%@\\376(\\300?m/\\247?]\\247\\333?\\030\\242%@}\\263\\320?k\\315\\343?\\304\\320\\243?T\\332*@\\243\\371\\033@SW)@X5\\031@\\030\\242%@?\\230\\302@\\311\\213\\347@\\322\\024\\331@\\217\\0220@\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Infer with TRITON server: AnomalyDetectionModel\n",
      "[INFO] Prediction shape: (1, 32, 4) \n",
      "[INFO] MAE & MSE loss calculated\n",
      "Output df:    MSE loss  MAE loss\n",
      "0  2.030342  1.091005 \n",
      "[INFO] Predictions performed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: \"AnomalyDetectionModel\"\n",
      "model_version: \"1\"\n",
      "outputs {\n",
      "  name: \"OutputLayer\"\n",
      "  datatype: \"FP32\"\n",
      "  shape: 1\n",
      "  shape: 32\n",
      "  shape: 4\n",
      "}\n",
      "raw_output_contents: \"\\236\\303\\222@UW\\270@)\\023\\274@\\376\\353\\326?Y\\\"B?:\\351z?)\\270r?\\366\\007\\305?\\345?]\\2761v\\227\\27690\\002\\277\\320|\\316?\\256|\\013\\276\\302\\365J\\276h\\265\\265\\2766U\\263?\\227,\\264=\\222\\322\\353>\\362y\\263>y\\033\\257?\\022\\311\\236@(h\\302@H&\\326@\\330\\361\\205?%\\262A\\276V&\\210=R\\261\\260>\\251\\237\\214?\\234G\\202>\\262-+?\\276\\0356?\\376\\243\\245?\\2202?\\277J\\317\\215\\275pj\\256\\276\\037\\334\\231?\\326#\\276\\273\\244c\\010?c\\303i>R\\'}?\\022\\307\\223@A\\253\\260@\\3731\\271@i\\022r?D\\340!\\277X\\223\\325\\274\\264Z\\273\\276U\\365\\216?\\267@\\224\\276x\\264\\376\\274\\257\\253p\\276te\\274?\\266\\3563>\\235)\\235>\\355\\0228?\\n\\312\\305?\\362w&?\\356;}?h\\351\\234?\\367|\\245?\\000T\\266>a\\267.?\\354\\353k?\\217}\\273?\\251\\233T?6\\341\\252?\\010q\\265?\\014`\\267?\\334ii?\\267\\036\\272?\\355\\356\\264?K\\177\\223?F\\301\\023@[K;@\\313\\2046@@\\216\\222?\\362$\\213@$\\307\\243@wP\\240@\\324\\273\\302?\\327\\355\\222?\\233\\262\\221?\\362\\204x?Yt\\002@)m\\256:\\177\\366\\242\\276\\201\\343:\\275\\242\\244\\357? CD?\\240\\202{?\\255l\\207?\\244\\376\\324?\\374_\\332>;9\\221>\\341{\\002?\\007a\\327?\\362^\\242?\\241w\\261?7?\\335?>\\223\\240??>\\241?k\\314\\266?\\356<\\257?WF\\254?\\266\\210\\311?<\\206\\371?G\\251\\377?^\\376\\231?0\\226\\202@3G\\227@9\\304\\241@t\\367\\265?\\210\\266\\276?t\\032\\341?\\024\\306\\357?\\210\\316\\245?Yh\\r@\\031\\374,@\\207\\354-@,\\262\\240?\\313\\312\\022@\\010S5@p\\342-@\\342\\244\\212?\\202\\347\\320?\\210\\033\\007@\\303\\366\\340?\\rpd?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if ( np.isnan(Instance).any() ):\n",
    "    if VERBOSE: logger.error('[ERROR] Instance contained NaN values')\n",
    "else:\n",
    "    df_response = inference(data          = Instance, \n",
    "                            model_name    = model_name, \n",
    "                            input_name    = input_name,\n",
    "                            output_name   = output_name,\n",
    "                            verbose       = VERBOSE);\n",
    "    \n",
    "    if VERBOSE: logger.info('[INFO] Predictions performed')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
