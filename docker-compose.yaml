version: '2'
volumes:
  model-weights-volume:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./model_repository/
services:
  # For Kube deployment with MIG check 
  # https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/
  triton-inference-server:
    image: nvcr.io/nvidia/tritonserver:22.03-py3  # download 21.12 version #21.12
    command: "/opt/tritonserver/bin/tritonserver --model-repository=/models"
    volumes:
      - model-weights-volume:/models
    ports:
      - 6000:8000
      - 6001:8001
      - 6002:8002